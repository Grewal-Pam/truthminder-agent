# ğŸ”„ TruthMindr ETL Pipeline

A modular Extract-Transform-Load (ETL) system for ingesting, cleaning, and enriching multimodal disinformation data from multiple sources.

---

## ğŸ“Š Data Sources

### Supported Ingest Channels:
1. **Reddit** - r/worldnews top posts via PRAW API
2. **NewsAPI** - Top headlines by country (requires `NEWSAPI_KEY`)
3. **CSV/TSV Folder** - Manual data drops into `data/raw/manual/`

---

## ğŸ—ï¸ Pipeline Architecture

The ETL follows a **layered medallion architecture**:

```
[Ingest Layer]
    â†“
[Transform Layer] â†’ Clean Text + Enrich Metadata + Deduplicate
    â†“
[Load Layer] â†’ DuckDB + Parquet (Bronze â†’ Silver â†’ Gold)
    â†“
[ML Enrichment] â†’ Add model predictions
```

### Pipeline Stages:

| Stage | Module | Description |
|-------|--------|-------------|
| **Ingest** | `ingest/fetch_*.py` | Fetch from Reddit, NewsAPI, or CSV folder |
| **Transform** | `transform/clean_text.py` | Tokenize, lowercase, remove URLs/special chars |
| **Enrich** | `transform/enrich_metadata.py` | Add timestamps, normalize scores, compute derived features |
| **Deduplicate** | `pipeline.py` | Remove duplicates on (id, source) |
| **Partition** | `load/to_parquet.py` | Store by source in Bronze layer |
| **Silver Layer** | `transform/to_silver.py` | Standardized schema across sources |
| **Gold Layer** | `ml/enrich_with_models.py` | Predictions from CLIP, ViLT, FLAVA |
| **Store** | `load/to_duckdb.py` | Load final data into DuckDB |

---

## ğŸš€ Quick Start

### 1. Set Environment Variables
```bash
export NEWSAPI_KEY="YOUR_NEWSAPI_KEY_HERE"
```

### 2. Run the Pipeline
```bash
# Ingest 50 posts from each source
python -m etl.pipeline

# Or with custom limits
python -c "from etl.pipeline import run; run(limit=100, country='gb')"
```

### 3. Verify Data Loaded
```bash
python - << 'EOF'
import duckdb
con = duckdb.connect('truthmindr.db')
print("ğŸ“Š Source distribution:")
print(con.execute("SELECT source, COUNT(*) as count FROM posts GROUP BY source").fetchdf())
print("\nğŸ“ Recent posts:")
print(con.execute("SELECT id, title, source FROM posts ORDER BY rowid DESC LIMIT 5").fetchdf())
con.close()
EOF
```

---

## ğŸ“ Module Structure

```
etl/
â”œâ”€â”€ __init__.py              # ETL module marker
â”œâ”€â”€ pipeline.py              # Main orchestration (run function)
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ fetch_reddit.py      # Reddit API client
â”‚   â”œâ”€â”€ fetch_newsapi.py     # NewsAPI client
â”‚   â””â”€â”€ fetch_csv_folder.py  # CSV/TSV directory reader
â”œâ”€â”€ transform/
â”‚   â”œâ”€â”€ clean_text.py        # Text normalization
â”‚   â”œâ”€â”€ enrich_metadata.py   # Feature engineering
â”‚   â””â”€â”€ to_silver.py         # Standardization layer
â”œâ”€â”€ load/
â”‚   â”œâ”€â”€ to_duckdb.py         # DuckDB writer
â”‚   â””â”€â”€ to_parquet.py        # Parquet partitioner
â”œâ”€â”€ ml/
â”‚   â””â”€â”€ enrich_with_models.py # Model predictions (CLIP, ViLT, FLAVA)
â”œâ”€â”€ flows/
â”‚   â””â”€â”€ etl_flow.py          # Prefect flow definition
â”œâ”€â”€ cleanup.sh               # Clear cached data
â””â”€â”€ README.md                # This file
```

---

## âš™ï¸ Configuration

### Environment Variables:
- `NEWSAPI_KEY` - Required for NewsAPI source
- `PRAW_CLIENT_ID`, `PRAW_CLIENT_SECRET` - Optional: Reddit API credentials (uses public API by default)

### Pipeline Parameters:
```python
from etl.pipeline import run

# Ingest 100 posts per source from US
run(limit=100, country="us")

# Ingest 200 posts from UK
run(limit=200, country="gb")
```

---

## ğŸ“Š Output Schema

Final DuckDB table (`posts`) contains:

| Column | Type | Source |
|--------|------|--------|
| `id` | VARCHAR | Post ID (unique per source) |
| `title` | VARCHAR | Post headline |
| `clean_title` | VARCHAR | Normalized title |
| `image_url` | VARCHAR | Image URL (if available) |
| `source` | VARCHAR | 'reddit', 'newsapi', 'csv' |
| `timestamp` | TIMESTAMP | Publication date |
| `upvote_ratio` | FLOAT | Normalized engagement metric |
| `score` | FLOAT | Post score/votes |
| `num_comments` | INT | Comment count |
| `clip_pred` | VARCHAR | CLIP prediction (if enriched) |
| `vilt_pred` | VARCHAR | ViLT prediction (if enriched) |
| `flava_pred` | VARCHAR | FLAVA prediction (if enriched) |

---

## ğŸ”§ Troubleshooting

### NewsAPI returns empty results:
- Verify `NEWSAPI_KEY` is set: `echo $NEWSAPI_KEY`
- Check API quota: [newsapi.org/account](https://newsapi.org/account)

### Reddit ingest fails:
- Public API doesn't require credentials
- Check network connectivity

### Out of memory during ML enrichment:
- Reduce post limit: `run(limit=50)`
- Process in batches manually

### Clear cached data:
```bash
bash etl/cleanup.sh
```

---

## ğŸš€ Integration with Airflow

See `airflow_dags/truthmindr_etl_dag.py` for scheduled daily runs.
