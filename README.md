# ğŸ§  TruthMinder-Agent  
**A Multimodal Disinformation Detection System using CLIP, ViLT, and FLAVA**

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-red)](https://streamlit.io/)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow)](https://huggingface.co/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸš€ Overview
**TruthMinder-Agent** is an agentic AI system designed to detect **misinformation** in online news posts using a **multimodal pipeline** that analyzes both **text and images**, supported by **metadata** and **logical consistency checks**.

This project extends my **Master's Thesis in Web and Data Science (University of Koblenz, 2025)** â€” accepted for publication at **ACM DHOW 2025 (Dublin)** â€” and integrates the best-performing models (CLIP, ViLT, FLAVA) into an interactive **Streamlit dashboard**.

---

## ğŸ§© Core Features
- **Multimodal Fusion** of text, image, and post metadata  
- **Ensemble of Pretrained Models** â€“ CLIP, ViLT, and FLAVA  
- **Evidence Layer** â€“ OCR-based text extraction from images  
- **Consistency Layer** â€“ Natural Language Inference (NLI) for logical checks  
- **Arbiter Module** â€“ Final decision fusion with abstain/uncertain handling  
- **Interactive UI** â€“ Streamlit-based visualization and test interface  

---

## ğŸ§­ System Architecture

The TruthMinder-Agent follows a modular agentic workflow consisting of four main reasoning stages:

```mermaid
flowchart LR
    A["ğŸ§  Perception<br/>CLIP Â· ViLT Â· FLAVA"] --> B["ğŸ“œ Evidence<br/>OCR Extraction"]
    B --> C["ğŸ” Consistency<br/>NLI + CLIP Similarity"]
    C --> D["âš–ï¸ Arbiter<br/>Decision Fusion"]
    D --> E["âœ… Final Label<br/>(Real Â· Satire/Mixed Â· Fake)"]
```

---

## ğŸ§© Core Components

The TruthMinder-Agent project is structured into clear modular layers to ensure transparency, maintainability, and reproducibility.

| Folder / File | Description |
|----------------|-------------|
| **agent/** | Core orchestration logic for the multimodal AI agent. Contains `runner.py`, which manages the Perception â†’ Evidence â†’ Consistency â†’ Arbiter workflow. |
| **models/** | Inference scripts for the three pretrained transformer models â€” **CLIP**, **ViLT**, and **FLAVA** â€” each adapted for binary (2-way) and ternary (3-way) disinformation classification. |
| **tools/** | Utility modules for auxiliary reasoning: **OCR extraction**, **NLI consistency checking**, **cosine similarity**, and **explainability** utilities. |
| **app/** | Streamlit-based user interface where users can upload, analyze, and visualize results interactively. |
| **evaluate/** | Scripts to regenerate and evaluate model metrics (accuracy, F1-score, precision, recall, Cohen's Kappa) from checkpoints. |
| **results/** | Stores evaluation plots such as confusion matrices, ROC and Precision-Recall curves for CLIP, ViLT, and FLAVA models. |
| **outputs/** | Contains final agent outputs â€” including `enriched.csv` (batch predictions) and JSON traces for each processed post. |
| **training/** | Training utilities including learning rate finder, trainers, and evaluators used during model fine-tuning. |
| **datasets/** | Preprocessing and dataset construction scripts for the Fakeddit dataset. Handles text cleaning, label encoding, and metadata normalization. |
| **runs/** | TensorBoard-compatible experiment logs for model training sessions. |
| **requirements.txt** | Python dependencies required to run the project. |
| **README.md** | This documentation file describing the project's structure and purpose. |
| **etl/** | Extract-Transform-Load pipeline for data ingestion from Reddit, NewsAPI, and CSV sources. |
| **airflow_dags/** | Apache Airflow DAG definitions for automated ETL orchestration. |

---

## ğŸ§  Highlights
- Modularized architecture enables individual testing of CLIP, ViLT, and FLAVA.
- Each layer (Perception â†’ Evidence â†’ Consistency â†’ Arbiter) is fully traceable through JSON logs.
- Enables both **single-post analysis** (interactive UI) and **batch evaluation** (CLI mode).
- Integrated ETL pipeline for automated data ingestion and enrichment.

---

## âš™ï¸ Installation & Setup

> ğŸ§© **Recommended**: Use a clean Conda environment with Python 3.9 + PyTorch + Transformers

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/Grewal-Pam/truthminder-agent.git
cd truthminder-agent

# 2ï¸âƒ£ Create & activate a new Conda environment
conda create -n truthminder python=3.9 -y
conda activate truthminder

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Verify setup
python -c "from tools import ocr; print('âœ… OCR module import OK')"
```

---

## ğŸš€ Running the Agentic Pipeline

> ğŸ’¡ TruthMinder-Agent supports three execution modes:  
> 1ï¸âƒ£ **Interactive UI (Streamlit)**  
> 2ï¸âƒ£ **Background Server Mode (for cloud/VMs)**  
> 3ï¸âƒ£ **Batch Mode (offline dataset processing)**

---

### â–¶ï¸ **Option 1 â€” Interactive UI (Streamlit)**

Launch the full web dashboard locally:

```bash
streamlit run app/test_app.py --server.port 8501
```

Then open your browser at ğŸ‘‰ **http://localhost:8501**

This allows you to:
- Upload an image + caption pair
- View predictions from CLIP, ViLT, and FLAVA
- Inspect OCR, NLI, and Arbiter reasoning layers
- See the final disinformation label with model confidence
- Explore visualizations of prediction traces

---

### â–¶ï¸ **Option 2 â€” Background Server Mode (ğŸ–§ for Cloud/VMs)**

Run the Streamlit app as a background process â€” ideal for remote servers.

```bash
nohup streamlit run app/test_app.py --server.port 8501 > truthmindr.log 2>&1 &
```

Monitor live logs:
```bash
tail -f truthmindr.log
```

Stop the server when finished:
```bash
pkill -f streamlit
```

---

### â–¶ï¸ **Option 3 â€” Batch Mode (ğŸ“¦ Offline Dataset Processing)**

Run the agent pipeline in non-interactive mode to process an entire dataset:

```bash
python agent/runner.py
```

This will generate:
- `outputs/enriched.csv` â†’ consolidated predictions for all posts
- `outputs/traces/*.json` â†’ per-post reasoning traces with all model outputs

Use this mode for large-scale evaluation, experiments, or model benchmarking.

---

## ğŸ–¼ï¸ Example Output & Visualization

When you analyze a post, the agent produces both a **final label** and a **reasoning trace** showing how each layer contributed to the decision.

### ğŸ§¾ Example Output (from `outputs/traces/c0xdqy.json`)

```json
{
  "post_id": "c0xdqy",
  "final_label": "Real",
  "final_confidence": 0.86,
  "nli_label": "NEUTRAL_NO_OCR",
  "consistency_score": 0.5886,
  "clip_cos": 0.6477,
  "ocr_text": "",
  "upvote_ratio": "-0.69",
  "score": "-0.11",
  "num_comments": "-0.12"
}
```

### ğŸ§  Interpretation

- **Perception Layer:** CLIP + ViLT + FLAVA predict probabilities for Real, Satire/Mixed, and Fake.
- **Evidence Layer:** No OCR text detected (ocr_text = "").
- **Consistency Layer:** NLI judged the imageâ€“caption relation as neutral.
- **Arbiter Layer:** Aggregated model probabilities â†’ highest confidence = Real (0.86).

Each analyzed post is saved as:
- `outputs/enriched.csv` â†’ all predictions in one file
- `outputs/traces/<post_id>.json` â†’ detailed reasoning trace

---

## ğŸ“Š Evaluation Results & Performance Summary

The TruthMinder-Agent integrates three multimodal transformer backbones â€” **CLIP**, **ViLT**, and **FLAVA** â€” fine-tuned on the Fakeddit dataset for 2-way and 3-way disinformation classification tasks, both **with** and **without metadata**.

| Model | Task | Accuracy | F1-Score | Cohen's Kappa | Metadata Used | Notes |
|:------|:-----|:---------:|:--------:|:--------------:|:--------------:|:------|
| **CLIP** | 2-way | 0.924 | 0.921 | 0.843 | âœ… | Robust visualâ€“text alignment |
| **CLIP** | 3-way | 0.802 | 0.796 | 0.684 | âœ… | Best visual reasoning performance |
| **ViLT** | 2-way | 0.887 | 0.885 | 0.769 | âœ… | Fastest inference, moderate accuracy |
| **ViLT** | 3-way | 0.752 | 0.742 | 0.621 | âœ… | Balanced multimodal fusion |
| **FLAVA** | 2-way | 0.901 | 0.896 | 0.812 | âœ… | Best metadata sensitivity |
| **FLAVA** | 3-way | 0.786 | 0.775 | 0.657 | âœ… | Most balanced across modalities |

> ğŸ“˜ *These results are drawn from Parminder Grewal's master's thesis (University of Koblenz, 2025) and correspond to the official ACM DHOW 2025 submission.*

---

### ğŸ¯ Key Insights
- Metadata (upvote ratio, score, comments) consistently improved all model metrics.  
- CLIP achieved the highest F1 in visual-heavy samples.  
- FLAVA provided the most stable metadata fusion.  
- ViLT offered lightweight, efficient inference for real-time scenarios.

---

### ğŸ“ˆ Visualization Samples

| Model | 2-way ROC Curve | 3-way Confusion Matrix |
|:------|:----------------:|:----------------------:|
| **CLIP** | ![CLIP ROC](results/CLIP/test_2_way_roc_curve.png) | ![CLIP CM](results/CLIP/test_3_way_confusion_matrix.png) |
| **ViLT** | ![ViLT ROC](results/VILT/3-way_classification_val_metrics.json) | ![ViLT CM](results/VILT/3-way_classification_test_metrics.json) |
| **FLAVA** | ![FLAVA ROC](results/FLAVA/3-way_classification_model.pth) | ![FLAVA CM](results/images/validation_3_way_confusion_matrix.png) |

> *(If running locally, the `results/` folder contains all plots and metrics JSON files for detailed inspection.)*

---

## ğŸš€ Future Directions

TruthMinder-Agent currently implements a full multimodal disinformation detection pipeline with OCR and NLI-based reasoning.  
Next planned enhancements include:

- **Verifier LLM Integration** â†’ adds final human-like judgment based on trace data.  
- **Retrieval-Augmented Generation (RAG)** â†’ links suspicious posts to credible news sources for evidence-backed verification.  
- **Explainability Module** â†’ highlights image/text regions influencing the model's decision.  
- **Riskâ€“Coverage & Calibration** â†’ measures model confidence and abstention reliability.  
- **FastAPI Deployment** â†’ enables lightweight API-based inference for integration into external products.

> ğŸ§© Long-term vision: evolve TruthMinder-Agent into a **self-explaining, evidence-aware disinformation verification assistant** capable of cross-modal reasoning and traceable outputs.

---

## ğŸ§¾ Citation

If you use or build upon this work, please cite:

```bibtex
@inproceedings{Grewal2025,
   author = {Parminder Kaur Grewal and Marina Ernst and Frank Hopfgartner},
   city = {New York, NY, USA},
   doi = {10.1145/3746275.3762205},
   isbn = {9798400720574},
   booktitle = {Proceedings of the 2nd International Workshop on Diffusion of Harmful Content on Online Web},
   month = {10},
   pages = {75-83},
   publisher = {ACM},
   title = {Beyond Text: Leveraging Vision-Language Models for Misinformation Detection},
   url = {https://dl.acm.org/doi/10.1145/3746275.3762205},
   year = {2025}
}
```

---

## ğŸ“š Additional Resources

- **ETL Pipeline:** See [etl/README.md](etl/README.md) for data ingestion and processing.
- **Airflow Orchestration:** See [airflow_dags/README.md](airflow_dags/README.md) for scheduled pipeline automation.
- **Master's Thesis:** Available at [ACM DHOW 2025 Proceedings](https://dl.acm.org/doi/10.1145/3746275.3762205)

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Author:** Parminder Kaur Grewal  
**Institution:** University of Koblenz, Germany  
**Last Updated:** December 2, 2025

