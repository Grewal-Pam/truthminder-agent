# ğŸ§  TruthMinder-Agent  
**A Multimodal Disinformation Detection System using CLIP, ViLT, and FLAVA**

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-red)](https://streamlit.io/)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow)](https://huggingface.co/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

### ğŸš€ Overview
**TruthMinder-Agent** is an agentic AI system designed to detect **misinformation** in online news posts using a **multimodal pipeline** that analyzes both **text and images**, supported by **metadata** and **logical consistency checks**.

This project extends Parminder Grewalâ€™s **Masterâ€™s Thesis in Web and Data Science (University of Koblenz, 2025)** â€” accepted for publication at **ACM DHOW 2025 (Dublin)** â€” and integrates the best-performing models (CLIP, ViLT, FLAVA) into an interactive **Streamlit dashboard**.

---

### ğŸ§© Core Features
- **Multimodal Fusion** of text, image, and post metadata  
- **Ensemble of Pretrained Models** â€“ CLIP, ViLT, and FLAVA  
- **Evidence Layer** â€“ OCR-based text extraction from images  
- **Consistency Layer** â€“ Natural Language Inference (NLI) for logical checks  
- **Arbiter Module** â€“ Final decision fusion with abstain/uncertain handling  
- **Interactive UI** â€“ Streamlit-based visualization and test interface  

---
## ğŸ§­ System Architecture

The TruthMinder-Agent follows a modular **agentic workflow** consisting of four main reasoning stages:

```mermaid
flowchart LR
    A["ğŸ§  Perception<br/>CLIP Â· ViLT Â· FLAVA"] --> B["ğŸ“œ Evidence<br/>OCR Extraction"]
    B --> C["ğŸ” Consistency<br/>NLI + CLIP Similarity"]
    C --> D["âš–ï¸ Arbiter<br/>Decision Fusion"]
    D --> E["âœ… Final Label<br/>(Real Â· Satire/Mixed Â· Fake)"]

## ğŸ§© Core Components

The TruthMinder-Agent project is structured into clear modular layers to ensure transparency, maintainability, and reproducibility.

| Folder / File | Description |
|----------------|-------------|
| **agent/** | Core orchestration logic for the multimodal AI agent. Contains `runner.py`, which manages the Perception â†’ Evidence â†’ Consistency â†’ Arbiter workflow. |
| **models/** | Inference scripts for the three pretrained transformer models â€” **CLIP**, **ViLT**, and **FLAVA** â€” each adapted for binary (2-way) and ternary (3-way) disinformation classification. |
| **tools/** | Utility modules for auxiliary reasoning: **OCR extraction**, **NLI consistency checking**, **cosine similarity**, and **explainability** utilities. |
| **app/** | Streamlit-based user interface where users can upload, analyze, and visualize results interactively. |
| **evaluate/** | Scripts to regenerate and evaluate model metrics (accuracy, F1-score, precision, recall, Cohenâ€™s Kappa) from checkpoints. |
| **results/** | Stores evaluation plots such as confusion matrices, ROC and Precision-Recall curves for CLIP, ViLT, and FLAVA models. |
| **outputs/** | Contains final agent outputs â€” including `enriched.csv` (batch predictions) and JSON traces for each processed post. |
| **training/** | Training utilities including learning rate finder, trainers, and evaluators used during model fine-tuning. |
| **datasets/** | Preprocessing and dataset construction scripts for the Fakeddit dataset. Handles text cleaning, label encoding, and metadata normalization. |
| **runs/** | TensorBoard-compatible experiment logs for model training sessions. |
| **requirements.txt** | Python dependencies required to run the project. |
| **README.md** | This documentation file describing the projectâ€™s structure and purpose. |

---

### ğŸ§  Highlights
- Modularized architecture enables individual testing of CLIP, ViLT, and FLAVA.
- Each layer (Perception â†’ Evidence â†’ Consistency â†’ Arbiter) is fully traceable through JSON logs.
- Enables both **single-post analysis** (interactive UI) and **batch evaluation** (CLI mode).

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/parmindergrewal/truthminder-agent.git
cd truthminder-agent

2ï¸âƒ£ Create and Activate a Conda Environment

Itâ€™s recommended to use a fresh Python 3.9 environment for compatibility with PyTorch and Hugging Face Transformers.

conda create -n truthminder python=3.9 -y
conda activate truthminder

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Verify Installation
python -c "from tools import ocr; print('âœ… OCR module import OK')"


You should see:

âœ… OCR module import OK


ğŸ§  Running the Agentic Pipeline
â–¶ï¸ Option 1: Run via Streamlit (Interactive UI)

Launch the complete web app locally:

streamlit run app/test_app.py --server.port 8501


Once launched, open your browser and go to:
ğŸ‘‰ http://localhost:8501

This allows you to:

Upload a post (image + caption)

View predictions from CLIP, ViLT, and FLAVA

Inspect OCR, NLI, and Arbiter reasoning

See the final disinformation label with confidence

â–¶ï¸ Option 2: Run in Background (Server Mode)

Useful for remote VMs or cloud environments:

nohup streamlit run app/test_app.py --server.port 8501 > truthmindr.log 2>&1 &


Check logs:

tail -f truthmindr.log


Stop the process:

pkill -f streamlit

â–¶ï¸ Option 3: Batch Mode (Offline Evaluation)

To process a file containing multiple posts:

python agent/runner.py


This will create:

outputs/enriched.csv â€” enriched predictions

outputs/traces/*.json â€” per-post reasoning trace files